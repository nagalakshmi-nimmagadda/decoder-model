{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8859b08994ca4217bfe71d33cc58c4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_469d4086daf04d299c9ffd9afe0a3e7d",
              "IPY_MODEL_9259443485b84bdba9bcf3bbb3ae3cb0",
              "IPY_MODEL_fac64bd041f24b8196a96e3782ea0ed7"
            ],
            "layout": "IPY_MODEL_864545c4fc7e4f9ca45b5a5ba89375d4"
          }
        },
        "469d4086daf04d299c9ffd9afe0a3e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce40c1f32d1e4c90abf4c362662a1916",
            "placeholder": "​",
            "style": "IPY_MODEL_1a2f40fc18a24a9f9efc16e56f8b9219",
            "value": "Epoch 1/10: 100%"
          }
        },
        "9259443485b84bdba9bcf3bbb3ae3cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d1def90ffa145bc94b444f39bf38b34",
            "max": 8195,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7e84817a7af44c988e0bfb42cec7599",
            "value": 8192
          }
        },
        "fac64bd041f24b8196a96e3782ea0ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e405c2b08d504c37af234c7ba4d70b10",
            "placeholder": "​",
            "style": "IPY_MODEL_ff4ec401f76744f9a3f836cf6338fbc3",
            "value": " 8192/8195 [38:51&lt;00:00,  8.69it/s, loss=0.0908, lr=3.00e-05]"
          }
        },
        "864545c4fc7e4f9ca45b5a5ba89375d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce40c1f32d1e4c90abf4c362662a1916": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2f40fc18a24a9f9efc16e56f8b9219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d1def90ffa145bc94b444f39bf38b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e84817a7af44c988e0bfb42cec7599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e405c2b08d504c37af234c7ba4d70b10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff4ec401f76744f9a3f836cf6338fbc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc5994b147b44ea18bf58a2f1276c830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d50e7d7d78c4c6184ae22bbc808c90d",
              "IPY_MODEL_add78b956cca4539b88bb12248757318",
              "IPY_MODEL_17fa5a28a11a444d9f8f5fa13a9d5454"
            ],
            "layout": "IPY_MODEL_63da55ec6827400f9ae19fd452b7f1e5"
          }
        },
        "0d50e7d7d78c4c6184ae22bbc808c90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37bf15dbbfad45bda983f7929107623d",
            "placeholder": "​",
            "style": "IPY_MODEL_00b91844003b4af88968712c0ea6aa5e",
            "value": "Epoch 2/10: 100%"
          }
        },
        "add78b956cca4539b88bb12248757318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07bcb8ec4d0e4c0f9a5f2db64e32f176",
            "max": 8195,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85370ec2e7004f1b92d96954d2e2d26b",
            "value": 8192
          }
        },
        "17fa5a28a11a444d9f8f5fa13a9d5454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fac2f2a95f84615a8d1ddb0100c7377",
            "placeholder": "​",
            "style": "IPY_MODEL_6cf41d966c3240c1a0360f74499b89f0",
            "value": " 8192/8195 [20:37&lt;00:00,  8.75it/s, loss=0.0897, lr=6.00e-05]"
          }
        },
        "63da55ec6827400f9ae19fd452b7f1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37bf15dbbfad45bda983f7929107623d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00b91844003b4af88968712c0ea6aa5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07bcb8ec4d0e4c0f9a5f2db64e32f176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85370ec2e7004f1b92d96954d2e2d26b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fac2f2a95f84615a8d1ddb0100c7377": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf41d966c3240c1a0360f74499b89f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LuVa3MMThYA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch transformers datasets tokenizers wandb tqdm numpy huggingface-hub accelerate gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - Imports and Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from typing import Optional, List\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import os\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "import jsonpointer\n",
        "import json\n",
        "\n",
        "# Clear GPU memory and cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "@contextmanager\n",
        "def nullcontext():\n",
        "    yield"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO_sbfVoUCzD",
        "outputId": "d0f9584d-fd17-4561-ae84-21fc9d02c1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_seq_length: int = 512):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_seq_length, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: [batch_size, seq_len, d_model]\"\"\"\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output, _ = self.self_attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 1024,\n",
        "                 n_layers: int = 12,\n",
        "                 n_heads: int = 16,\n",
        "                 d_ff: int = 4096,\n",
        "                 max_seq_length: int = 256,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.final_layer = nn.Linear(d_model, vocab_size)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.01)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            nn.init.normal_(layer.self_attention.in_proj_weight, mean=0.0, std=0.01)\n",
        "            nn.init.normal_(layer.self_attention.out_proj.weight, mean=0.0, std=0.01)\n",
        "\n",
        "            for name, param in layer.ff.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.normal_(param, mean=0.0, std=0.01)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "        nn.init.normal_(self.final_layer.weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.final_layer.bias)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Create causal mask if not provided\n",
        "        if mask is None:\n",
        "            seq_length = x.size(1)\n",
        "            mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
        "            mask = mask.to(x.device)\n",
        "\n",
        "        x = self.token_embedding(x)\n",
        "        x = x.transpose(0, 1)  # Convert to sequence-first format\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(0, 1)  # Convert back to batch-first\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "\n",
        "        output = self.final_layer(x)\n",
        "        return output\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_path: str, device: str = 'cpu'):\n",
        "        \"\"\"Load a pretrained model from a directory\"\"\"\n",
        "        try:\n",
        "            # Load config\n",
        "            config_path = os.path.join(model_path, \"config.json\")\n",
        "            if not os.path.exists(config_path):\n",
        "                raise FileNotFoundError(f\"Config not found at {config_path}\")\n",
        "\n",
        "            with open(config_path) as f:\n",
        "                config = json.load(f)\n",
        "\n",
        "            # Create model instance\n",
        "            model = cls(\n",
        "                vocab_size=config['vocab_size'],\n",
        "                d_model=config['d_model'],\n",
        "                n_layers=config['n_layers'],\n",
        "                n_heads=config['n_heads'],\n",
        "                d_ff=config['d_ff'],\n",
        "                max_seq_length=config['max_seq_length'],\n",
        "                dropout=config.get('dropout', 0.1)\n",
        "            )\n",
        "\n",
        "            # Load weights\n",
        "            weights_path = os.path.join(model_path, \"pytorch_model.bin\")\n",
        "            if not os.path.exists(weights_path):\n",
        "                raise FileNotFoundError(f\"Weights not found at {weights_path}\")\n",
        "\n",
        "            state_dict = torch.load(weights_path, map_location=device)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "            return model.to(device)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error loading model from {model_path}: {str(e)}\")\n",
        "\n",
        "    def generate(self, tokenizer, prompt, max_length=100, temperature=0.5, device='cpu', top_k=20):\n",
        "      \"\"\"Generate text from a prompt\"\"\"\n",
        "      self.eval()\n",
        "\n",
        "      # Tokenize the prompt\n",
        "      tokens = tokenizer.encode(prompt).ids\n",
        "      input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for _ in range(max_length):\n",
        "              # Forward pass to get logits\n",
        "              outputs = self(input_ids)\n",
        "              next_token_logits = outputs[:, -1, :].float()\n",
        "\n",
        "              # Scale logits by temperature\n",
        "              next_token_logits = next_token_logits / temperature\n",
        "\n",
        "              # Get the top_k tokens and their probabilities\n",
        "              probs = torch.softmax(next_token_logits, dim=-1)\n",
        "              probs, indices = torch.topk(probs, top_k, dim=-1)\n",
        "\n",
        "              # Normalize probabilities\n",
        "              probs = probs / torch.sum(probs, dim=-1, keepdim=True)\n",
        "\n",
        "              # Sample from the top_k probabilities\n",
        "              next_token = indices[0, torch.multinomial(probs[0], num_samples=1).item()].unsqueeze(0)\n",
        "\n",
        "              # Append the sampled token to the input sequence\n",
        "              input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "              # Stop if the EOS token is generated\n",
        "              if next_token.item() == tokenizer.token_to_id(\"[EOS]\"):\n",
        "                  break\n",
        "\n",
        "      # Decode the generated sequence\n",
        "      return tokenizer.decode(input_ids[0].tolist())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wPxxuLKHUKO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], tokenizer, max_length: int = 256, stride: int = 128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        print(\"Encoding texts...\")\n",
        "        self.encoded_texts = self._encode_texts(texts)\n",
        "\n",
        "    def _encode_texts(self, texts):\n",
        "        encoded = []\n",
        "        for text in texts:\n",
        "            tokens = self.tokenizer.encode(text).ids\n",
        "            if len(tokens) < self.max_length:\n",
        "                tokens = tokens + [self.tokenizer.token_to_id(\"[PAD]\")] * (self.max_length - len(tokens))\n",
        "            else:\n",
        "                tokens = tokens[:self.max_length]\n",
        "            encoded.append(tokens)\n",
        "        return encoded\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.encoded_texts[idx])\n",
        "\n",
        "def create_tokenizer(texts, vocab_size=50000):\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
        "        min_frequency=2\n",
        "    )\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    tokenizer.train_from_iterator(texts, trainer)\n",
        "    return tokenizer\n",
        "\n",
        "def save_tokenizer(tokenizer, path: str):\n",
        "    \"\"\"Save tokenizer to disk\"\"\"\n",
        "    tokenizer.save(f\"{path}/tokenizer.json\")\n",
        "\n",
        "def load_tokenizer(path: str):\n",
        "    \"\"\"Load tokenizer from disk\"\"\"\n",
        "    return Tokenizer.from_file(f\"{path}/tokenizer.json\")"
      ],
      "metadata": {
        "id": "xHMzE7Q5VOw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        val_dataloader: Optional = None,\n",
        "        lr: float = 3e-4,\n",
        "        device = None,\n",
        "        gradient_accumulation_steps: int = 32\n",
        "    ):\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Training on device: {self.device}\")\n",
        "\n",
        "        self.model = model.to(self.device)\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "\n",
        "        # Optimizer setup\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=lr,\n",
        "            weight_decay=0.01,\n",
        "            eps=1e-8,\n",
        "            betas=(0.9, 0.95)\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        num_training_steps = len(train_dataloader) * 50 // gradient_accumulation_steps\n",
        "        num_warmup_steps = num_training_steps // 5\n",
        "\n",
        "        self.scheduler = get_cosine_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.best_loss = float('inf')\n",
        "        self.scaler = torch.amp.GradScaler('cuda') if self.device.type == 'cuda' else None\n",
        "\n",
        "    def _compute_loss(self, batch):\n",
        "      x = batch.to(self.device)\n",
        "\n",
        "      # Create causal mask\n",
        "      seq_length = x.size(1)\n",
        "      mask = (torch.triu(torch.ones(seq_length, seq_length)) == 1).transpose(0, 1)\n",
        "      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "      mask = mask.to(self.device)\n",
        "\n",
        "      with torch.amp.autocast('cuda') if self.device.type == 'cuda' else nullcontext():\n",
        "          logits = self.model(x, mask=mask)\n",
        "          shift_logits = logits[:, :-1, :].contiguous()\n",
        "          shift_labels = x[:, 1:].contiguous()\n",
        "\n",
        "          # Fix: Use -100 as default ignore_index for padding\n",
        "          loss = F.cross_entropy(\n",
        "              shift_logits.view(-1, shift_logits.size(-1)),\n",
        "              shift_labels.view(-1),\n",
        "              label_smoothing=0.1,\n",
        "              ignore_index=-100  # Fixed: Use -100 as default padding index\n",
        "          )\n",
        "\n",
        "      return loss\n",
        "\n",
        "    def save_checkpoint(self, epoch, loss, path='checkpoints'):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'loss': loss,\n",
        "            'train_losses': self.train_losses\n",
        "        }\n",
        "        torch.save(checkpoint, f'{path}/checkpoint_epoch_{epoch}.pt')\n",
        "\n",
        "    def train(self, epochs=50):\n",
        "        print(\"\\nStarting training...\")\n",
        "        wandb.init(project=\"transformer-training\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0\n",
        "\n",
        "            pbar = tqdm(total=len(self.train_dataloader),\n",
        "                       desc=f\"Epoch {epoch+1}/{epochs}\",\n",
        "                       leave=True)\n",
        "\n",
        "            running_loss = 0\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            for batch_idx, batch in enumerate(self.train_dataloader, 1):\n",
        "                try:\n",
        "                    loss = self._compute_loss(batch)\n",
        "                    scaled_loss = loss / self.gradient_accumulation_steps\n",
        "\n",
        "                    self.scaler.scale(scaled_loss).backward()\n",
        "\n",
        "                    if batch_idx % self.gradient_accumulation_steps == 0:\n",
        "                        self.scaler.unscale_(self.optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
        "\n",
        "                        self.scaler.step(self.optimizer)\n",
        "                        self.scaler.update()\n",
        "                        self.scheduler.step()\n",
        "                        self.optimizer.zero_grad()\n",
        "\n",
        "                        running_loss += loss.item()\n",
        "                        avg_loss = running_loss / self.gradient_accumulation_steps\n",
        "                        self.train_losses.append(avg_loss)\n",
        "\n",
        "                        wandb.log({\n",
        "                            'loss': avg_loss,\n",
        "                            'learning_rate': self.scheduler.get_last_lr()[0]\n",
        "                        })\n",
        "\n",
        "                        pbar.set_postfix({\n",
        "                            'loss': f'{avg_loss:.4f}',\n",
        "                            'lr': f'{self.scheduler.get_last_lr()[0]:.2e}'\n",
        "                        })\n",
        "                        pbar.update(self.gradient_accumulation_steps)\n",
        "\n",
        "                        epoch_loss += running_loss\n",
        "                        running_loss = 0\n",
        "\n",
        "                        if batch_idx % 500 == 0:\n",
        "                            torch.cuda.empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if \"out of memory\" in str(e):\n",
        "                        print(f\"\\nOOM error in batch {batch_idx}. Skipping...\")\n",
        "                        if hasattr(torch.cuda, 'empty_cache'):\n",
        "                            torch.cuda.empty_cache()\n",
        "                        continue\n",
        "                    else:\n",
        "                        raise e\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / len(self.train_dataloader)\n",
        "            print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "            print(f\"Average Loss: {avg_epoch_loss:.4f}\")\n",
        "            print(f\"Learning Rate: {self.scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "            if avg_epoch_loss < self.best_loss:\n",
        "                self.best_loss = avg_epoch_loss\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': avg_epoch_loss,\n",
        "                }, 'best_model.pt')\n",
        "                print(\"► New best model saved!\")\n",
        "\n",
        "            if avg_epoch_loss < 0.099999:\n",
        "                print(f\"\\n✓ Reached target loss!\")\n",
        "                break"
      ],
      "metadata": {
        "id": "lhOi3wftVTK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading data...\")\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "texts = [t.strip() for t in text.split('\\n') if t.strip()]\n",
        "print(f\"Loaded {len(texts)} text segments\")\n",
        "\n",
        "print(\"\\nCreating tokenizer...\")\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
        ")\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train_from_iterator(texts, trainer)\n",
        "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "# Save tokenizer\n",
        "print(\"Saving tokenizer...\")\n",
        "tokenizer.save(\"tokenizer.json\")\n",
        "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "print(\"\\nTesting tokenizer...\")\n",
        "test_text = texts[0][:100]\n",
        "encoded = tokenizer.encode(test_text)\n",
        "decoded = tokenizer.decode(encoded.ids)\n",
        "print(f\"Original: {test_text}\")\n",
        "print(f\"Decoded : {decoded}\")\n",
        "\n",
        "print(\"\\nCreating dataset...\")\n",
        "dataset = TextDataset(texts, tokenizer, max_length=256, stride=128)\n",
        "print(f\"Created dataset with {len(dataset)} sequences\")\n",
        "\n",
        "# Create train/val split\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(\"\\nCreating dataloaders...\")\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"\\nInitializing model...\")\n",
        "model = TransformerDecoder(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    d_model=1024,\n",
        "    n_layers=12,\n",
        "    n_heads=16,\n",
        "    d_ff=4096,\n",
        "    max_seq_length=256\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model initialized with {total_params:,} parameters\")\n",
        "print(\"Verifying random initialization...\")\n",
        "print(f\"Embedding mean: {model.token_embedding.weight.mean().item():.4f}\")\n",
        "print(f\"Embedding std: {model.token_embedding.weight.std().item():.4f}\")\n",
        "\n",
        "trainer = TransformerTrainer(\n",
        "    model=model,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=None,\n",
        "    lr=3e-4,\n",
        "    device=device,\n",
        "    gradient_accumulation_steps=16\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train(epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964,
          "referenced_widgets": [
            "8859b08994ca4217bfe71d33cc58c4bb",
            "469d4086daf04d299c9ffd9afe0a3e7d",
            "9259443485b84bdba9bcf3bbb3ae3cb0",
            "fac64bd041f24b8196a96e3782ea0ed7",
            "864545c4fc7e4f9ca45b5a5ba89375d4",
            "ce40c1f32d1e4c90abf4c362662a1916",
            "1a2f40fc18a24a9f9efc16e56f8b9219",
            "8d1def90ffa145bc94b444f39bf38b34",
            "a7e84817a7af44c988e0bfb42cec7599",
            "e405c2b08d504c37af234c7ba4d70b10",
            "ff4ec401f76744f9a3f836cf6338fbc3",
            "fc5994b147b44ea18bf58a2f1276c830",
            "0d50e7d7d78c4c6184ae22bbc808c90d",
            "add78b956cca4539b88bb12248757318",
            "17fa5a28a11a444d9f8f5fa13a9d5454",
            "63da55ec6827400f9ae19fd452b7f1e5",
            "37bf15dbbfad45bda983f7929107623d",
            "00b91844003b4af88968712c0ea6aa5e",
            "07bcb8ec4d0e4c0f9a5f2db64e32f176",
            "85370ec2e7004f1b92d96954d2e2d26b",
            "1fac2f2a95f84615a8d1ddb0100c7377",
            "6cf41d966c3240c1a0360f74499b89f0"
          ]
        },
        "id": "4ltDpdJFVXCb",
        "outputId": "424a799b-d26c-486d-e3df-ba2d1683b0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded 32777 text segments\n",
            "\n",
            "Creating tokenizer...\n",
            "Vocabulary size: 18150\n",
            "Saving tokenizer...\n",
            "Vocabulary size: 18150\n",
            "\n",
            "Testing tokenizer...\n",
            "Original: First Citizen:\n",
            "Decoded : First Citizen :\n",
            "\n",
            "Creating dataset...\n",
            "Encoding texts...\n",
            "Created dataset with 32777 sequences\n",
            "\n",
            "Creating dataloaders...\n",
            "\n",
            "Initializing model...\n",
            "Model initialized with 188,344,038 parameters\n",
            "Verifying random initialization...\n",
            "Embedding mean: 0.0000\n",
            "Embedding std: 0.0100\n",
            "Training on device: cuda\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250120_105205-ix9ox119</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nagalakshmin444-the-school-of-ai/transformer-training/runs/ix9ox119' target=\"_blank\">glorious-field-17</a></strong> to <a href='https://wandb.ai/nagalakshmin444-the-school-of-ai/transformer-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nagalakshmin444-the-school-of-ai/transformer-training' target=\"_blank\">https://wandb.ai/nagalakshmin444-the-school-of-ai/transformer-training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nagalakshmin444-the-school-of-ai/transformer-training/runs/ix9ox119' target=\"_blank\">https://wandb.ai/nagalakshmin444-the-school-of-ai/transformer-training/runs/ix9ox119</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/10:   0%|          | 0/8195 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8859b08994ca4217bfe71d33cc58c4bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "Average Loss: 0.1356\n",
            "Learning Rate: 3.00e-05\n",
            "► New best model saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/10:   0%|          | 0/8195 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc5994b147b44ea18bf58a2f1276c830"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "Average Loss: 0.0913\n",
            "Learning Rate: 6.00e-05\n",
            "► New best model saved!\n",
            "\n",
            "✓ Reached target loss!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2A2UZozveUVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - Verify Training Results\n",
        "import os\n",
        "\n",
        "def verify_training():\n",
        "    \"\"\"Verify that training completed successfully and files exist\"\"\"\n",
        "    print(\"Verifying training results...\")\n",
        "\n",
        "    # 1. Check if best_model.pt exists and load it\n",
        "    if not os.path.exists('best_model.pt'):\n",
        "        raise FileNotFoundError(\"best_model.pt not found! Training may not have completed.\")\n",
        "\n",
        "    checkpoint = torch.load('best_model.pt', map_location='cpu')\n",
        "    print(f\"\\n✓ Found best_model.pt\")\n",
        "    print(f\"  • Best loss: {checkpoint['loss']:.4f}\")\n",
        "    print(f\"  • Saved at epoch: {checkpoint['epoch']}\")\n",
        "\n",
        "    # 2. Check if tokenizer.json exists\n",
        "    if not os.path.exists('tokenizer.json'):\n",
        "        raise FileNotFoundError(\"tokenizer.json not found! Tokenizer was not saved.\")\n",
        "\n",
        "    # Load tokenizer to verify it works\n",
        "    test_tokenizer = Tokenizer.from_file('tokenizer.json')\n",
        "    vocab_size = test_tokenizer.get_vocab_size()\n",
        "    print(f\"\\n✓ Found tokenizer.json\")\n",
        "    print(f\"  • Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    # 3. Test model generation\n",
        "    print(\"\\nTesting model generation...\")\n",
        "    model = TransformerDecoder(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=1024,\n",
        "        n_layers=12,\n",
        "        n_heads=16,\n",
        "        d_ff=4096,\n",
        "        max_seq_length=256\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    test_prompt = \"First Citizen\"\n",
        "    try:\n",
        "        generated = model.generate(test_tokenizer, test_prompt, max_length=100,top_k=50,device='cpu')\n",
        "        print(f\"\\n✓ Model generation test successful\")\n",
        "        print(f\"  • Input: {test_prompt}\")\n",
        "        print(f\"  • Output: {generated}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Model generation test failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Run verification\n",
        "is_ready = verify_training()\n",
        "\n",
        "if is_ready:\n",
        "    print(\"\\n✅ All checks passed! Ready for deployment.\")\n",
        "else:\n",
        "    print(\"\\n❌ Please fix the issues before deploying.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAlzV060Via-",
        "outputId": "bd613a17-c31f-4446-d29a-91a66217520d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying training results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-4326e149dfd9>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('best_model.pt', map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Found best_model.pt\n",
            "  • Best loss: 0.0913\n",
            "  • Saved at epoch: 1\n",
            "\n",
            "✓ Found tokenizer.json\n",
            "  • Vocabulary size: 18150\n",
            "\n",
            "Testing model generation...\n",
            "\n",
            "✓ Model generation test successful\n",
            "  • Input: First Citizen\n",
            "  • Output: First Citizen :\n",
            "\n",
            "✅ All checks passed! Ready for deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - Verify and Deploy\n",
        "import os\n",
        "import json\n",
        "from huggingface_hub import HfApi, create_repo, login\n",
        "import shutil\n",
        "\n",
        "def prepare_and_deploy(username=\"ninagala\", model_repo_name=\"shakespeare-model\", space_repo_name=\"shakespeare-app\"):\n",
        "    \"\"\"Prepare and deploy model and demo\"\"\"\n",
        "    print(\"Starting deployment process...\")\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(\"model_repo\", exist_ok=True)\n",
        "    os.makedirs(\"space_repo\", exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # 1. Verify files exist\n",
        "        if not os.path.exists('best_model.pt'):\n",
        "            raise FileNotFoundError(\"best_model.pt not found!\")\n",
        "        if not os.path.exists('tokenizer.json'):\n",
        "            raise FileNotFoundError(\"tokenizer.json not found!\")\n",
        "\n",
        "        # 2. Prepare model files\n",
        "        print(\"\\nPreparing model files...\")\n",
        "\n",
        "        # Load checkpoint\n",
        "        checkpoint = torch.load('best_model.pt', map_location='cpu', weights_only=True)\n",
        "\n",
        "        # Save config\n",
        "        config = {\n",
        "            'vocab_size': tokenizer.get_vocab_size(),\n",
        "            'd_model': 1024,\n",
        "            'n_layers': 12,\n",
        "            'n_heads': 16,\n",
        "            'd_ff': 4096,\n",
        "            'max_seq_length': 256,\n",
        "            'dropout': 0.1,\n",
        "            'best_loss': checkpoint['loss']\n",
        "        }\n",
        "\n",
        "        with open(\"model_repo/config.json\", \"w\") as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "\n",
        "        # Save model weights\n",
        "        torch.save(checkpoint['model_state_dict'], \"model_repo/pytorch_model.bin\")\n",
        "\n",
        "        # Copy tokenizer\n",
        "        shutil.copy('tokenizer.json', \"model_repo/tokenizer.json\")\n",
        "\n",
        "        # 3. Create Gradio demo files\n",
        "        print(\"Creating Gradio files...\")\n",
        "\n",
        "        # Update app.py with correct model path\n",
        "        with open(\"space_repo/app.py\", \"w\") as f:\n",
        "            f.write(f\"\"\"\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tokenizers import Tokenizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_seq_length: int = 512):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_seq_length, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output, _ = self.self_attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 1024,\n",
        "                 n_layers: int = 12,\n",
        "                 n_heads: int = 16,\n",
        "                 d_ff: int = 4096,\n",
        "                 max_seq_length: int = 256,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.final_layer = nn.Linear(d_model, vocab_size)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.01)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            nn.init.normal_(layer.self_attention.in_proj_weight, mean=0.0, std=0.01)\n",
        "            nn.init.normal_(layer.self_attention.out_proj.weight, mean=0.0, std=0.01)\n",
        "\n",
        "            for name, param in layer.ff.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.normal_(param, mean=0.0, std=0.01)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "        nn.init.normal_(self.final_layer.weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.final_layer.bias)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Create causal mask if not provided\n",
        "        if mask is None:\n",
        "            seq_length = x.size(1)\n",
        "            mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
        "            mask = mask.to(x.device)\n",
        "\n",
        "        x = self.token_embedding(x)\n",
        "        x = x.transpose(0, 1)  # Convert to sequence-first format\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(0, 1)  # Convert back to batch-first\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "\n",
        "        output = self.final_layer(x)\n",
        "        return output\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_id: str, device: str = 'cpu'):\n",
        "        config_file = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
        "        with open(config_file) as f:\n",
        "            config = json.load(f)\n",
        "        model = cls(\n",
        "            vocab_size=config['vocab_size'],\n",
        "            d_model=config['d_model'],\n",
        "            n_layers=config['n_layers'],\n",
        "            n_heads=config['n_heads'],\n",
        "            d_ff=config['d_ff'],\n",
        "            max_seq_length=config['max_seq_length'],\n",
        "            dropout=config.get('dropout', 0.1)\n",
        "        )\n",
        "        weights_file = hf_hub_download(repo_id=model_id, filename=\"pytorch_model.bin\")\n",
        "        state_dict = torch.load(weights_file, map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        return model.to(device)\n",
        "\n",
        "\n",
        "def generate_text(prompt, max_length=100, temperature=0.7):\n",
        "    try:\n",
        "        # Load model and tokenizer from Hugging Face Hub\n",
        "        model_id = \"{username}/{model_repo_name}\"\n",
        "        tokenizer_file = hf_hub_download(repo_id=model_id, filename=\"tokenizer.json\")\n",
        "\n",
        "        model = TransformerDecoder.from_pretrained(model_id)\n",
        "        tokenizer = Tokenizer.from_file(tokenizer_file)\n",
        "\n",
        "        # Generate text\n",
        "        model.eval()\n",
        "        tokens = tokenizer.encode(prompt).ids\n",
        "        input_ids = torch.tensor(tokens).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                outputs = model(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                if next_token.item() == tokenizer.token_to_id(\"[EOS]\"):\n",
        "                    break\n",
        "\n",
        "        return tokenizer.decode(input_ids[0].tolist())\n",
        "    except Exception as e:\n",
        "        return f\"Error: {{str(e)}}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=3, placeholder=\"Enter your prompt here...\", label=\"Prompt\"),\n",
        "        gr.Slider(20, 200, value=100, step=1, label=\"Maximum Length\"),\n",
        "        gr.Slider(0.1, 2.0, value=0.7, label=\"Temperature\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Text\"),\n",
        "    title=\"Shakespeare Text Generator\",\n",
        "    description=\"Generate Shakespeare-style text using a transformer decoder.\",\n",
        "    examples=[\n",
        "        [\"To be, or not to be\"],\n",
        "        [\"Friends, Romans, countrymen\"],\n",
        "        [\"Now is the winter of our discontent\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "\"\"\")\n",
        "\n",
        "        # Create requirements.txt\n",
        "        with open(\"space_repo/requirements.txt\", \"w\") as f:\n",
        "            f.write(\"\"\"\n",
        "torch>=2.0.0\n",
        "transformers>=4.30.0\n",
        "gradio>=3.40.0\n",
        "tokenizers>=0.14.0\n",
        "huggingface-hub>=0.16.4\n",
        "\"\"\".strip())\n",
        "\n",
        "        # 4. Deploy to Hugging Face\n",
        "        print(\"\\nDeploying to Hugging Face...\")\n",
        "\n",
        "        # Login\n",
        "        HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
        "        login(token=HF_TOKEN)\n",
        "\n",
        "        api = HfApi()\n",
        "\n",
        "        # Push model\n",
        "        print(\"\\nPushing model...\")\n",
        "        model_repo = f\"{username}/{model_repo_name}\"\n",
        "        create_repo(model_repo, exist_ok=True)\n",
        "        api.upload_folder(\n",
        "            folder_path=\"model_repo\",\n",
        "            repo_id=model_repo,\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "        print(f\"✓ Model pushed to: https://huggingface.co/{model_repo}\")\n",
        "\n",
        "        # Push space\n",
        "        print(\"\\nPushing Gradio demo...\")\n",
        "        space_repo = f\"{username}/{space_repo_name}\"\n",
        "        create_repo(space_repo, repo_type=\"space\", space_sdk=\"gradio\", exist_ok=True)\n",
        "        api.upload_folder(\n",
        "            folder_path=\"space_repo\",\n",
        "            repo_id=space_repo,\n",
        "            repo_type=\"space\"\n",
        "        )\n",
        "        print(f\"✓ Demo pushed to: https://huggingface.co/spaces/{space_repo}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error during deployment: {str(e)}\")\n",
        "        raise e\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        shutil.rmtree(\"model_repo\", ignore_errors=True)\n",
        "        shutil.rmtree(\"space_repo\", ignore_errors=True)\n",
        "\n",
        "# Execute deployment\n",
        "prepare_and_deploy(\n",
        "    username=\"ninagala\",\n",
        "    model_repo_name=\"shakespeare-model-decoder\",  # Updated name\n",
        "    space_repo_name=\"shakespeare-decoder-app\"     # Updated name\n",
        ")\n",
        "\n",
        "# https://huggingface.co/ninagala/shakespeare-model-decoder/tree/main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAMlEM-_eMk-",
        "outputId": "b85b6b98-9392-4293-b7b6-9b6cc4590de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting deployment process...\n",
            "\n",
            "Preparing model files...\n",
            "Creating Gradio files...\n",
            "\n",
            "Deploying to Hugging Face...\n",
            "Enter your Hugging Face token: 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\n",
            "\n",
            "Pushing model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model pushed to: https://huggingface.co/ninagala/shakespeare-model-decoder\n",
            "\n",
            "Pushing Gradio demo...\n",
            "✓ Demo pushed to: https://huggingface.co/spaces/ninagala/shakespeare-decoder-app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wAgMnjS5lx5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}